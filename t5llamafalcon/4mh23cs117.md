# ğŸ¤– T5, LLaMA & Falcon â€” Complete Guide + 15 Interview Questions

> A comprehensive reference for understanding three landmark Large Language Models (LLMs): **T5**, **LLaMA**, and **Falcon** â€” with architecture breakdowns, comparisons, resources, and interview prep.

---

## ğŸ“Œ Table of Contents

1. [T5 â€” Text-to-Text Transfer Transformer](#t5)
2. [LLaMA â€” Large Language Model Meta AI](#llama)
3. [Falcon â€” Technology Innovation Institute Model](#falcon)
4. [Model Comparison Table](#comparison)
5. [Visual Architecture Diagrams](#visuals)
6. [ğŸ¥ Video Resources](#videos)
7. [ğŸ–¼ï¸ Architecture Images](#images)
8. [ğŸ¯ 15 Interview Questions & Answers](#interview)
9. [Quick Reference Cheat Sheet](#cheatsheet)

---

## <a name="t5"></a>1. ğŸŸ¢ T5 â€” Text-to-Text Transfer Transformer

### What is T5?

T5 (Text-to-Text Transfer Transformer) was introduced by **Google Research** in 2019 through the paper [*"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"*](https://arxiv.org/abs/1910.10683). The core innovation of T5 is its **unified framework**: every NLP task is reformulated as a text-to-text problem.

```
Input:  "translate English to French: The cat sat on the mat."
Output: "Le chat s'est assis sur le tapis."

Input:  "summarize: [long article text here]"
Output: "A brief summary of the article."

Input:  "sentiment: The movie was absolutely terrible."
Output: "negative"
```

### ğŸ—ï¸ Architecture

T5 uses a **standard encoder-decoder Transformer** architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              T5 Architecture                 â”‚
â”‚                                              â”‚
â”‚  Input Tokens                                â”‚
â”‚       â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   ENCODER STACK     â”‚  â† Bidirectional   â”‚
â”‚  â”‚  (Self-Attention)   â”‚    Context          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚          â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   DECODER STACK     â”‚  â† Autoregressive  â”‚
â”‚  â”‚ (Cross + Self Attn) â”‚    Generation      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚          â”‚                                   â”‚
â”‚     Output Tokens                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key architectural features:**
- **Relative position embeddings** instead of absolute
- **Layer normalization** before attention, not after
- **No biases** in feed-forward layers
- Trained on **C4 (Colossal Cleaned Crawled Corpus)**

### ğŸ“Š T5 Model Variants

| Model       | Parameters | Layers | Heads | d_model |
|-------------|-----------|--------|-------|---------|
| T5-Small    | 60M       | 6      | 8     | 512     |
| T5-Base     | 220M      | 12     | 12    | 768     |
| T5-Large    | 770M      | 24     | 16    | 1024    |
| T5-XL       | 3B        | 24     | 32    | 2048    |
| T5-XXL      | 11B       | 24     | 64    | 4096    |
| Flan-T5-XXL | 11B       | 24     | 64    | 4096    |

### âœ… Key Strengths
- Unified text-to-text framework handles **any NLP task**
- Excellent for **fine-tuning** on downstream tasks
- Strong benchmark performance (GLUE, SuperGLUE, SQuAD)
- **Flan-T5** variant improves with instruction tuning

### âŒ Limitations
- **Encoder-decoder** means higher memory requirements than decoder-only
- Not as efficient for pure generation tasks as GPT-style models
- Requires task prefixes (slightly more prompt engineering)

### ğŸ”— Official Resources
- ğŸ“„ [Original Paper (arXiv)](https://arxiv.org/abs/1910.10683)
- ğŸ¤— [HuggingFace T5 Models](https://huggingface.co/t5-base)
- ğŸ™ [Google T5 GitHub](https://github.com/google-research/text-to-text-transfer-transformer)
- ğŸ“Š [Flan-T5 Paper](https://arxiv.org/abs/2210.11416)

---

## <a name="llama"></a>2. ğŸ¦™ LLaMA â€” Large Language Model Meta AI

### What is LLaMA?

LLaMA was released by **Meta AI** in February 2023, followed by **LLaMA 2** in July 2023 and **LLaMA 3** in 2024. It is a **decoder-only** autoregressive language model designed to be highly efficient at smaller scales â€” making it accessible to researchers and enterprises without massive compute.

> *"We release a collection of language models ranging from 7B to 65B parameters trained on publicly available datasets."* â€” Meta AI Research

### ğŸ—ï¸ Architecture

LLaMA is based on the **GPT-style decoder-only Transformer** with several key modifications:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             LLaMA Architecture               â”‚
â”‚                                              â”‚
â”‚  Input Tokens                                â”‚
â”‚       â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Embedding Layer                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚       â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Transformer Block (Ã—N)               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  RMSNorm â†’ Self-Attention (GQA) â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  RoPE Embeddings               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  RMSNorm â†’ SwiGLU FFN          â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚       â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RMSNorm â†’ Linear â†’ Softmax          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚       â”‚                                      â”‚
â”‚  Output Logits                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key LLaMA Innovations:**
- **RMSNorm** (Root Mean Square Normalization) instead of LayerNorm â†’ faster
- **RoPE** (Rotary Positional Embeddings) â†’ better long-context generalization
- **SwiGLU** activation function instead of ReLU â†’ better performance
- **GQA** (Grouped Query Attention) in LLaMA 2/3 â†’ faster inference
- **Pre-normalization** â†’ training stability

### ğŸ“Š LLaMA Model Variants

| Model       | Parameters | Context Length | Training Tokens |
|-------------|-----------|----------------|-----------------|
| LLaMA 1 7B  | 7B        | 2,048          | 1T              |
| LLaMA 1 65B | 65B       | 2,048          | 1.4T            |
| LLaMA 2 7B  | 7B        | 4,096          | 2T              |
| LLaMA 2 70B | 70B       | 4,096          | 2T              |
| LLaMA 3 8B  | 8B        | 8,192          | 15T             |
| LLaMA 3 70B | 70B       | 8,192          | 15T             |
| LLaMA 3.1 405B | 405B  | 128K           | 15.6T           |

### âœ… Key Strengths
- **Open weights** â€” downloadable for research and commercial use
- Excellent performance-per-parameter ratio
- Strong community ecosystem (Ollama, llama.cpp, LM Studio)
- **RLHF fine-tuned** variants (Llama-2-Chat) for assistant use
- Can run on **consumer hardware** (7B models on 8GB VRAM)

### âŒ Limitations
- Earlier versions required applying for access
- Context window limited compared to commercial models (until Llama 3.1)
- Requires significant compute for 70B+ variants

### ğŸ”— Official Resources
- ğŸ“„ [LLaMA Paper (arXiv)](https://arxiv.org/abs/2302.13971)
- ğŸ“„ [LLaMA 2 Paper](https://arxiv.org/abs/2307.09288)
- ğŸ¤— [HuggingFace LLaMA Models](https://huggingface.co/meta-llama)
- ğŸŒ [Meta AI Blog](https://ai.meta.com/llama/)
- ğŸ™ [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)

---

## <a name="falcon"></a>3. ğŸ¦… Falcon â€” Technology Innovation Institute

### What is Falcon?

Falcon was developed by the **Technology Innovation Institute (TII)** in Abu Dhabi, UAE. Released in 2023, it became one of the top-performing open-source models and was notable for being licensed under the **Apache 2.0** license, allowing commercial use with fewer restrictions than LLaMA.

> Falcon-40B topped the HuggingFace Open LLM Leaderboard in mid-2023, beating many larger models.

### ğŸ—ï¸ Architecture

Falcon uses a **decoder-only** architecture with several custom efficiency optimizations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Falcon Architecture               â”‚
â”‚                                              â”‚
â”‚  Input Tokens                                â”‚
â”‚       â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Embedding + ALiBi Positional Bias    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚       â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Falcon Block (Ã—N)                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  LayerNorm                      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€ Multi-Query Attention (MQA) â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€ Parallel MLP               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (Attention + MLP run PARALLEL) â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚       â”‚                                      â”‚
â”‚  Linear â†’ Softmax â†’ Output                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Falcon Innovations:**
- **Multi-Query Attention (MQA)** â€” single key/value head shared across all query heads â†’ massive speedup
- **Parallel Attention + MLP** â€” both run simultaneously, not sequentially â†’ higher throughput
- **FlashAttention** â€” memory-efficient attention implementation
- **ALiBi** (Attention with Linear Biases) positional encoding â†’ better length extrapolation
- Trained on **RefinedWeb** â€” a high-quality web dataset with extensive deduplication

### ğŸ“Š Falcon Model Variants

| Model        | Parameters | Context | License    |
|--------------|-----------|---------|------------|
| Falcon-7B    | 7B        | 2,048   | Apache 2.0 |
| Falcon-40B   | 40B       | 2,048   | Apache 2.0 |
| Falcon-180B  | 180B      | 2,048   | Custom*    |
| Falcon-7B-Instruct  | 7B  | 2,048  | Apache 2.0 |
| Falcon-40B-Instruct | 40B | 2,048  | Apache 2.0 |
| Falcon2-11B  | 11B       | 4,096   | Apache 2.0 |

*Falcon-180B has a custom license allowing commercial use with conditions.

### âœ… Key Strengths
- **Apache 2.0 license** â†’ most permissive open-source option
- **Multi-Query Attention** makes inference very fast
- Excellent benchmark scores relative to size
- **RefinedWeb** training data is extremely high quality
- Strong multilingual capabilities

### âŒ Limitations
- Smaller community ecosystem vs. LLaMA
- Context window limited at 2K (original variants)
- Less emphasis on RLHF/instruction fine-tuning

### ğŸ”— Official Resources
- ğŸ“„ [RefinedWeb Paper](https://arxiv.org/abs/2306.01116)
- ğŸ¤— [HuggingFace Falcon Models](https://huggingface.co/tiiuae/falcon-40b)
- ğŸŒ [TII Official Website](https://www.tii.ae/falcon)
- ğŸ™ [Falcon GitHub](https://huggingface.co/tiiuae)

---

## <a name="comparison"></a>4. ğŸ“Š Model Comparison Table

| Feature              | T5              | LLaMA              | Falcon             |
|----------------------|-----------------|--------------------|--------------------|
| **Creator**          | Google Research | Meta AI            | TII Abu Dhabi      |
| **Year**             | 2019            | 2023               | 2023               |
| **Architecture**     | Encoder-Decoder | Decoder-Only       | Decoder-Only       |
| **Paradigm**         | Text-to-Text    | Autoregressive     | Autoregressive     |
| **License**          | Apache 2.0      | Custom/Llama 2/3   | Apache 2.0         |
| **Best Use Case**    | Fine-tuning, NLP tasks | Generation, Chat | Fast generation   |
| **Positional Encoding** | Relative    | RoPE               | ALiBi              |
| **Normalization**    | LayerNorm       | RMSNorm            | LayerNorm          |
| **Attention Type**   | Multi-Head      | GQA (v2/3)         | Multi-Query        |
| **Activation**       | ReLU/GEGLU      | SwiGLU             | GELU               |
| **Training Data**    | C4              | Mix (CommonCrawl)  | RefinedWeb         |
| **Max Params**       | 11B (T5-XXL)    | 405B (3.1)         | 180B               |
| **Commercial Use**   | âœ… Free          | âœ… (with limits)    | âœ… Free             |

---

## <a name="visuals"></a>5. ğŸ–¼ï¸ Architecture Images

### Transformer Architecture (Base)

```
                     TRANSFORMER
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   ENCODER   â”‚         â”‚   DECODER   â”‚
     â”‚             â”‚         â”‚             â”‚
 â”€â”€â”€â–ºâ”‚ Multi-Head  â”‚         â”‚ Masked MHA  â”‚â—„â”€â”€â”€ Output
     â”‚  Attention  â”‚    â”Œâ”€â”€â”€â–ºâ”‚  (Self)     â”‚     (shifted)
     â”‚             â”‚    â”‚    â”‚             â”‚
     â”‚  Feed Fwd   â”‚    â”‚    â”‚ Multi-Head  â”‚
     â”‚  Network    â”‚    â””â”€â”€â”€â”€â”‚  Attention  â”‚
     â”‚             â”‚  Memory â”‚  (Cross)    â”‚
     â”‚  Ã—N Layers  â”‚         â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  Feed Fwd  â”‚
                             â”‚             â”‚
                             â”‚  Ã—M Layers  â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     [T5 uses both]          [LLaMA/Falcon: Decoder only]
```

### Attention Mechanism Comparison

```
Multi-Head Attention (T5):           Multi-Query Attention (Falcon):
Q1 K1 V1 â†’ Head 1 â”€â”               Q1 Q2 Q3 Q4
Q2 K2 V2 â†’ Head 2  â”œâ”€â–º Concat       â”‚  â”‚  â”‚  â”‚
Q3 K3 V3 â†’ Head 3  â”‚   + Linear     â””â”€â”€â”¼â”€â”€â”¼â”€â”€â”˜
Q4 K4 V4 â†’ Head 4 â”€â”˜                   K  V        â† Shared!
                                    (single K, V for all Q)
                                    Much faster inference!

Grouped Query Attention (LLaMA 2/3):
Q1 Q2 â”‚ Q3 Q4
      â”‚
    K1 V1  K2 V2          â† Groups of queries
                             share K,V heads
```

---

## <a name="videos"></a>6. ğŸ¥ Video Resources

### T5 Videos
| Title | Channel | Link |
|-------|---------|------|
| T5: Text-to-Text Transfer Transformer | Yannic Kilcher | [â–¶ï¸ YouTube](https://www.youtube.com/watch?v=91iLu6OOrwk) |
| T5 Explained â€” The Unified NLP Framework | HuggingFace | [â–¶ï¸ YouTube](https://www.youtube.com/@huggingface) |
| Flan-T5 Deep Dive | AI Coffee Break | [â–¶ï¸ YouTube](https://www.youtube.com/@AICoffeeBreak) |

### LLaMA Videos
| Title | Channel | Link |
|-------|---------|------|
| LLaMA Explained: KV-Cache, Rotary Positional Embedding, RMS Norm | Umar Jamil | [â–¶ï¸ YouTube](https://www.youtube.com/watch?v=Mn_9W1nCFLo) |
| Building LLaMA from Scratch | Andrej Karpathy (style) | [â–¶ï¸ YouTube](https://www.youtube.com/@AndrejKarpathy) |
| LLaMA 2 â€” Meta's Open Source LLM | Prompt Engineering | [â–¶ï¸ YouTube](https://www.youtube.com/watch?v=zJBpRn2zTco) |
| Running LLaMA Locally with Ollama | NetworkChuck | [â–¶ï¸ YouTube](https://www.youtube.com/@NetworkChuck) |

### Falcon Videos
| Title | Channel | Link |
|-------|---------|------|
| Falcon LLM â€” Architecture Deep Dive | AI Explained | [â–¶ï¸ YouTube](https://www.youtube.com/@aiexplained-official) |
| Falcon-40B: The Best Open-Source LLM | Matthew Berman | [â–¶ï¸ YouTube](https://www.youtube.com/@matthew_berman) |
| Multi-Query Attention Explained | Yannic Kilcher | [â–¶ï¸ YouTube](https://www.youtube.com/@YannicKilcher) |

### General LLM Architecture Videos
| Title | Channel | Link |
|-------|---------|------|
| Attention Is All You Need (Paper Explained) | Yannic Kilcher | [â–¶ï¸ YouTube](https://www.youtube.com/watch?v=iDulhoQ2pro) |
| Let's build GPT: from scratch, in code | Andrej Karpathy | [â–¶ï¸ YouTube](https://www.youtube.com/watch?v=kCc8FmEb1nY) |
| The Illustrated Transformer | Jay Alammar | [â–¶ï¸ Blog + Video](https://jalammar.github.io/illustrated-transformer/) |
| LLM Visualization Interactive | [â–¶ï¸ Interactive](https://bbycroft.net/llm) |

---

## <a name="images"></a>7. ğŸ–¼ï¸ Architecture Reference Images

| Model | Image Resource |
|-------|---------------|
| T5 Architecture Diagram | [arXiv Paper Fig 1](https://arxiv.org/abs/1910.10683) |
| T5 Task Format Overview | [Google AI Blog](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) |
| LLaMA Architecture | [arXiv Paper Fig 1](https://arxiv.org/abs/2302.13971) |
| RoPE Embeddings Visual | [Eleuther AI Blog](https://blog.eleuther.ai/rotary-embeddings/) |
| Falcon Parallel Layers | [HuggingFace Blog](https://huggingface.co/blog/falcon) |
| GQA vs MQA vs MHA | [GQA Paper Fig 1](https://arxiv.org/abs/2305.13245) |
| Illustrated Transformer | [Jay Alammar Blog](https://jalammar.github.io/illustrated-transformer/) |
| RefinedWeb Data Pipeline | [RefinedWeb Paper](https://arxiv.org/abs/2306.01116) |

> ğŸ’¡ **Tip:** The [Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/) is the best visual resource for understanding all three model families.

> ğŸ’¡ **Interactive 3D Visualization:** [https://bbycroft.net/llm](https://bbycroft.net/llm) shows exactly how tokens flow through transformer layers.

---

## <a name="interview"></a>8. ğŸ¯ 15 Interview Questions & Answers

---

### Q1. What is the key innovation of T5 compared to earlier transformer models?

**Answer:**
T5's key innovation is its **"text-to-text" unified framework** â€” every NLP task (classification, summarization, translation, Q&A, etc.) is reframed as a sequence-to-sequence problem where both input and output are text strings. Unlike BERT (encoder-only, classification output) or GPT (decoder-only, generation), T5 uses a **full encoder-decoder architecture** and task prefixes like `"translate English to French:"` or `"summarize:"` to condition behavior. This allows a single model to be fine-tuned on any task with the same training procedure, loss function, and decoding strategy.

---

### Q2. Explain the difference between encoder-only, decoder-only, and encoder-decoder architectures. Which model belongs to which?

**Answer:**
- **Encoder-only (e.g., BERT):** Reads input bidirectionally. Best for understanding tasks like classification, NER, extractive QA. Output is a vector per token, not generated text.
- **Decoder-only (e.g., LLaMA, Falcon, GPT):** Reads left-to-right with causal masking. Ideal for **text generation** â€” predicts next token given previous. Simpler, faster inference.
- **Encoder-Decoder (e.g., T5, BART):** Encoder reads the full input, decoder generates output. Best for **seq2seq tasks** like translation, summarization, abstractive QA.

**Mapping:**
- T5 â†’ Encoder-Decoder
- LLaMA â†’ Decoder-only
- Falcon â†’ Decoder-only

---

### Q3. What is RoPE (Rotary Positional Embedding) and why does LLaMA use it?

**Answer:**
RoPE encodes positional information by **rotating the query and key vectors** in the attention mechanism by angles proportional to their positions. Mathematically, for position `m`, a vector `x` is rotated by `mÎ¸` in each 2D subspace.

**Why LLaMA uses it:**
1. **Relative positions emerge naturally** â€” the dot product of Q and K depends only on their relative position, not absolute positions
2. **Better length generalization** â€” can extrapolate to sequence lengths beyond training
3. **No extra parameters** â€” unlike learned absolute embeddings
4. **Compatible with KV-cache** â€” maintains efficiency during autoregressive generation

---

### Q4. What is Multi-Query Attention (MQA) in Falcon and how does it differ from standard Multi-Head Attention?

**Answer:**
In standard **Multi-Head Attention (MHA)**, each attention head has its **own separate Q, K, V** projection matrices:
- H heads â†’ H sets of Q, K, V

In **Multi-Query Attention (MQA)** (Falcon):
- Each head has its own **Q** projections
- But **all heads share a single K and V** projection
- Result: HÃ—Q, 1Ã—K, 1Ã—V

**Benefits:**
- Dramatically **reduces KV-cache memory** during inference (proportional to H)
- **Faster generation** â€” less data to read from memory per step
- Minimal quality degradation in practice

**Grouped Query Attention (GQA)** in LLaMA 2/3 is a middle ground: G groups of heads share K,V (1 < G < H).

---

### Q5. What is RMSNorm and why does LLaMA prefer it over LayerNorm?

**Answer:**
**LayerNorm** computes mean and variance, then normalizes:
```
LayerNorm(x) = Î³ Â· (x - Î¼) / âˆš(ÏƒÂ² + Îµ) + Î²
```

**RMSNorm** skips the mean subtraction (re-centering), only computing the root mean square:
```
RMSNorm(x) = Î³ Â· x / RMS(x),  where RMS(x) = âˆš(mean(xÂ²) + Îµ)
```

**Why LLaMA uses RMSNorm:**
- ~10â€“15% **faster computation** (no mean calculation)
- **Fewer parameters** (no bias Î²)
- Empirically achieves **similar or better performance** than LayerNorm
- Simpler gradient flow â†’ more stable training

---

### Q6. What training data was used for T5, LLaMA, and Falcon? How do they differ?

**Answer:**

| Model  | Training Data | Key Characteristics |
|--------|--------------|---------------------|
| T5     | **C4** (Colossal Cleaned Crawled Corpus) | ~750GB of cleaned Common Crawl; English-only |
| LLaMA 1| Mix: CommonCrawl, GitHub, Wikipedia, Books, ArXiv, StackExchange | ~1.4T tokens; multi-source |
| LLaMA 2| Same mix + more web data | ~2T tokens |
| LLaMA 3| Mix including synthetic data | ~15T tokens; very large |
| Falcon | **RefinedWeb** | TII's own high-quality filtered web corpus; aggressive dedup |

**Key difference:** Falcon's **RefinedWeb** is notable for its extreme deduplication and quality filtering, which TII argues produces better models per token than diverse multi-source corpora.

---

### Q7. What is the SwiGLU activation function and why is it used in LLaMA?

**Answer:**
SwiGLU is an activation function from Noam Shazeer's 2020 paper, combining **Swish** (smooth ReLU variant) with a **Gating** mechanism:

```
SwiGLU(x, W, V) = Swish(xW) âŠ™ (xV)
Swish(x) = x Â· sigmoid(x)
```

The gate `(xV)` allows the network to selectively pass or suppress information.

**Why LLaMA uses it:**
- Consistently **outperforms ReLU and GELU** on language modeling tasks
- Better gradient flow â†’ **faster training convergence**
- Effectively increases model capacity with the gating mechanism
- Used in PaLM, LaMDA, and LLaMA â€” proven at scale

**Trade-off:** SwiGLU FFN needs 3 weight matrices instead of 2 (Wâ‚, Wâ‚‚, V), so LLaMA reduces the hidden dimension to keep parameter count equivalent.

---

### Q8. How does Falcon's "parallel attention + MLP" differ from standard sequential transformer blocks?

**Answer:**
**Standard Transformer block (sequential):**
```
x â†’ LayerNorm â†’ Self-Attention â†’ Add â†’ LayerNorm â†’ FFN â†’ Add
```

**Falcon's parallel block:**
```
x â†’ LayerNorm â†’ Attention â”€â”
                           â”œâ”€â–º Add â†’ x_out
              â†’ MLP â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Both Attention and MLP operations receive the **same normalized input** and their outputs are **summed together** rather than run sequentially.

**Benefits:**
- **Higher throughput** â€” Attention and MLP can run in parallel on hardware
- Enables use of **tensor parallelism** more efficiently
- Slight memory savings during forward pass
- Empirically, performance is comparable to sequential while being faster

This design was also explored in models like PaLM and Falcon builds on it extensively.

---

### Q9. Compare LLaMA, Falcon, and T5 in terms of licensing and commercial usability.

**Answer:**

| Model | License | Commercial Use | Key Restrictions |
|-------|---------|----------------|-----------------|
| T5 / Flan-T5 | Apache 2.0 | âœ… Free | None |
| LLaMA 1 | Research only | âŒ No commercial | Meta research license |
| LLaMA 2 | Llama 2 Community | âœ… Under 700M MAU | Must display Meta attribution |
| LLaMA 3 | Llama 3 License | âœ… Commercial | Similar to LLaMA 2 |
| Falcon 7B/40B | Apache 2.0 | âœ… Fully free | None |
| Falcon 180B | Custom TII | âœ… With conditions | Must not compete with TII products in certain ways |

**Key insight:** Falcon (7B/40B) has the **most permissive license** among the three, while T5 is freely usable but smaller. LLaMA 2/3 allows commercial use but with attribution requirements.

---

### Q10. What is ALiBi positional encoding (used in Falcon) and how does it help with longer contexts?

**Answer:**
**ALiBi (Attention with Linear Biases)** â€” introduced in "Train Short, Test Long" (Press et al., 2022) â€” adds a **linear penalty** to attention scores based on distance between tokens, instead of positional embeddings:

```
Attention Score = QÂ·Káµ€/âˆšd - m Â· |i - j|
```
Where `m` is a head-specific slope and `|i-j|` is the distance between positions i and j.

**Advantages:**
1. **No positional embeddings** â€” no parameters, no embedding table
2. **Length extrapolation** â€” models trained on 2K context can often generalize to 4K+ at inference time
3. **Recency bias** â€” naturally attends more to recent tokens (often desirable)
4. **Simpler implementation** â€” just add a bias matrix to attention scores

**vs RoPE (LLaMA):** RoPE tends to perform better on exact retrieval tasks; ALiBi generalizes more gracefully to unseen lengths.

---

### Q11. How does fine-tuning differ between T5 and LLaMA/Falcon?

**Answer:**

**T5 Fine-tuning:**
- Add task prefix to input: `"classify: The food was great."` â†’ `"positive"`
- Fine-tune **both encoder and decoder** weights
- Use **cross-entropy loss** on output sequence
- Standard seq2seq training
- Works well with supervised datasets (input-output pairs)

**LLaMA/Falcon Fine-tuning approaches:**
1. **Full fine-tuning** â€” update all weights, expensive
2. **LoRA (Low-Rank Adaptation)** â€” inject trainable rank-decomposition matrices into attention layers; most popular
3. **QLoRA** â€” quantize base model to 4-bit, apply LoRA on top; fits on single GPU
4. **RLHF / DPO** â€” for instruction following and alignment (used for LLaMA-Chat variants)

**Key difference:** T5 is naturally suited for supervised fine-tuning due to its seq2seq structure. LLaMA/Falcon require careful prompt formatting for instruction following, but LoRA/QLoRA make them very accessible for fine-tuning on consumer hardware.

---

### Q12. What is the "Scaling Law" and how does it relate to T5, LLaMA, and Falcon design choices?

**Answer:**
Scaling laws (Kaplan et al. 2020, Hoffmann et al. "Chinchilla" 2022) describe the power-law relationship:

```
Loss âˆ C^(-Î±)  where C = compute budget
Optimal: N âˆ D (parameters scale with tokens)
Chinchilla ratio: ~20 tokens per parameter
```

**Impact on each model:**

- **T5:** Predates Chinchilla laws; models may be under-trained relative to their size
- **LLaMA 1:** Designed to train smaller models for longer â€” LLaMA-7B was trained on 1T tokens, making it more compute-optimal for inference than GPT-3 175B despite fewer parameters. Meta prioritized **inference efficiency** over training efficiency.
- **LLaMA 2/3:** Incorporates Chinchilla insights; LLaMA 3 uses ~15T tokens â€” heavily over-trains for better inference performance
- **Falcon:** RefinedWeb's high quality means each token is more informative; quality > quantity approach

**Key insight:** LLaMA's philosophy â€” "train smaller models longer" â€” made powerful models accessible on consumer hardware, a fundamental shift in the open-source AI ecosystem.

---

### Q13. Explain KV-Cache and why it matters for inference with LLaMA and Falcon.

**Answer:**
During autoregressive generation, the model computes attention over all previous tokens at each step. Without caching, this means recomputing K and V matrices for all previous tokens every step â€” O(nÂ²) complexity.

**KV-Cache** stores the Key and Value matrices from all previous tokens:
```
Step 1: Generate token 1, cache Kâ‚, Vâ‚
Step 2: Generate token 2, reuse Kâ‚,Vâ‚, compute only Kâ‚‚,Vâ‚‚
Step n: Generate token n, reuse all Kâ‚..Kâ‚™â‚‹â‚, Vâ‚..Vâ‚™â‚‹â‚
```

**Memory requirement:**
```
KV-Cache size = 2 Ã— num_layers Ã— num_kv_heads Ã— head_dim Ã— seq_len Ã— dtype_bytes
```

**Why MQA (Falcon) helps:**
- Standard 40B model KV-cache: ~40 heads Ã— ... = Very large
- MQA: Only 1 K,V pair per layer â†’ **dramatically smaller** KV cache
- Allows **longer contexts** and **larger batch sizes** at inference time

**Why GQA (LLaMA 2/3) is a good middle ground:**
- 8 KV groups instead of 40 heads â†’ ~5Ã— memory reduction
- Better quality than pure MQA while still being efficient

---

### Q14. How would you choose between T5, LLaMA, and Falcon for a production NLP application?

**Answer:**
The choice depends on your specific requirements:

**Choose T5/Flan-T5 when:**
- Your task is well-defined (translation, summarization, classification)
- You need a **smaller, fine-tunable model** (T5-Small at 60M is deployable anywhere)
- You want **structured output** (classification labels, extracted answers)
- You're working on tasks with **labeled training data**
- Budget is limited; inference must be cheap

**Choose LLaMA when:**
- You need **open-ended text generation** or a conversational assistant
- You want a **large ecosystem** of tools (Ollama, LM Studio, llama.cpp)
- You need the **largest parameter counts** for complex reasoning (70B, 405B)
- Your use case needs **instruction following** (use LLaMA-Chat variants)
- You want **community support and fine-tuned variants**

**Choose Falcon when:**
- You need **fast inference** at production scale (MQA advantage)
- You want the most **permissive open license** (Apache 2.0)
- Your application is **generation-heavy** with high throughput requirements
- You're building **commercial products** without Meta's attribution requirements

**General rule:** T5 for task-specific fine-tuning, LLaMA for best open-source performance, Falcon for speed + permissive licensing.

---

### Q15. What are the key differences between LLaMA 1, LLaMA 2, and LLaMA 3?

**Answer:**

| Feature | LLaMA 1 | LLaMA 2 | LLaMA 3 |
|---------|---------|---------|---------|
| Release | Feb 2023 | Jul 2023 | Apr 2024 |
| Sizes | 7B, 13B, 33B, 65B | 7B, 13B, 70B | 8B, 70B, 405B |
| Context | 2,048 | 4,096 | 8,192 (128K in 3.1) |
| Training Tokens | 1â€“1.4T | 2T | 15T |
| Attention | Multi-Head | GQA (70B only) | GQA (all sizes) |
| Vocabulary | 32K | 32K | 128K |
| License | Research | Commercial (with limits) | Commercial (with limits) |
| RLHF (Chat) | No | Yes (LLaMA-2-Chat) | Yes (LLaMA-3-Instruct) |
| Key Improvement | First open LLM | Longer context, better alignment | Massive data, GQA everywhere, huge vocab |

**LLaMA 3's 128K vocabulary** (vs 32K in v1/v2) is a significant improvement â€” better tokenization of code, multilingual text, and special characters, resulting in fewer tokens per document and more efficient training.

---

## <a name="cheatsheet"></a>9. ğŸ“‹ Quick Reference Cheat Sheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LLM QUICK CHEAT SHEET                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Feature    â”‚     T5       â”‚    LLaMA     â”‚   Falcon     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Architecture â”‚ Enc-Dec      â”‚ Dec-Only     â”‚ Dec-Only     â”‚
â”‚ Position Emb â”‚ Relative     â”‚ RoPE         â”‚ ALiBi        â”‚
â”‚ Normalizationâ”‚ LayerNorm    â”‚ RMSNorm      â”‚ LayerNorm    â”‚
â”‚ Activation   â”‚ ReLU/GEGLU   â”‚ SwiGLU       â”‚ GELU         â”‚
â”‚ Attention    â”‚ MHA          â”‚ GQA          â”‚ MQA          â”‚
â”‚ Attn+MLP     â”‚ Sequential   â”‚ Sequential   â”‚ Parallel     â”‚
â”‚ License      â”‚ Apache 2.0   â”‚ LLaMA 2/3    â”‚ Apache 2.0   â”‚
â”‚ Best For     â”‚ Fine-tuning  â”‚ Generation   â”‚ Fast Infer.  â”‚
â”‚ Training Dataâ”‚ C4           â”‚ Multi-source â”‚ RefinedWeb   â”‚
â”‚ Max Size     â”‚ 11B          â”‚ 405B         â”‚ 180B         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Equations to Remember

```python
# Attention Score
Attention(Q,K,V) = softmax(QKáµ€ / âˆšd_k) Â· V

# RoPE rotation
f(x, m) = x Â· e^(imÎ¸)  (complex representation)

# RMSNorm
RMSNorm(x) = x / RMS(x) * Î³,   RMS(x) = âˆš(mean(xÂ²))

# MQA (Falcon) - shared K,V across all heads
Headáµ¢ = Attention(Qáµ¢Wáµ¢á´¼, KW_kv, VW_kv)  # single K,V

# SwiGLU (LLaMA)
FFN(x) = (Swish(xWâ‚) âŠ™ xV) Â· Wâ‚‚
```

---

## ğŸ”— Additional Resources

### Papers
- ğŸ“„ [Attention Is All You Need (2017)](https://arxiv.org/abs/1706.03762) â€” Original Transformer
- ğŸ“„ [T5 Paper (2020)](https://arxiv.org/abs/1910.10683)
- ğŸ“„ [LLaMA Paper (2023)](https://arxiv.org/abs/2302.13971)
- ğŸ“„ [LLaMA 2 Paper (2023)](https://arxiv.org/abs/2307.09288)
- ğŸ“„ [Falcon/RefinedWeb (2023)](https://arxiv.org/abs/2306.01116)
- ğŸ“„ [RoPE Paper (2021)](https://arxiv.org/abs/2104.09864)
- ğŸ“„ [ALiBi Paper (2022)](https://arxiv.org/abs/2108.12409)
- ğŸ“„ [GQA Paper (2023)](https://arxiv.org/abs/2305.13245)

### Interactive Tools
- ğŸŒ [LLM Visualization](https://bbycroft.net/llm)
- ğŸŒ [Transformer Explainer](https://poloclub.github.io/transformer-explainer/)
- ğŸŒ [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- ğŸŒ [HuggingFace Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

### Blogs
- ğŸ“ [HuggingFace Falcon Blog](https://huggingface.co/blog/falcon)
- ğŸ“ [Meta LLaMA 2 Blog](https://ai.meta.com/blog/llama-2/)
- ğŸ“ [Google T5 Blog](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)

---

## ğŸ‘¥ Contributing

Feel free to open issues or PRs to:
- Add more interview questions
- Update model information as new versions release
- Add code examples for loading/fine-tuning these models
- Improve architecture diagrams

---

## ğŸ“œ License

This documentation is released under the **MIT License** â€” free to use, share, and modify with attribution.

---

*Last updated: February 2026 | Covers T5, LLaMA 1/2/3, Falcon 1/2*