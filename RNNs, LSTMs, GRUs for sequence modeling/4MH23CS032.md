# RNNs, LSTMs, GRUs for sequence modeling

1. What is sequence modeling?
    - Sequence modeling is used when data comes in order, like sentences, speech, or time-series data.
    - The model learns patterns while considering previous elements in the sequence.

2. What are RNNs, LSTMs, and GRUs?
    #### RNNs (Recurrent Neural Networks): 
    - RNNs are neural networks designed for sequential data (like text, speech, or time series).
    - They maintain a hidden state that carries information from previous time steps.

    #### LSTMs (Long Short-Term Memory):
    - LSTMs are a type of RNN that can capture long-term dependencies in data.

    #### GRUs (Gated Recurrent Units):
    - GRUs are a simpler variant of LSTMs that also capture long-term dependencies but with fewer parameters.

3. Why can't we use a normal neural network for sequences?
    - Normal neural networks treat inputs independently.
    - They do not remember previous inputs

4. How do RNNs work?
    - RNNs process sequences one element at a time, maintaining a hidden state that captures information about previous elements.
    - The hidden state is updated at each time step based on the current input and the previous hidden state.

5. What are the limitations of RNNs?
    - RNNs can struggle with long-term dependencies due to the vanishing gradient problem, where gradients become very small during backpropagation, making it difficult for the model to learn from earlier time steps.    

6. How do LSTMs and GRUs address the limitations of RNNs?
    - LSTMs and GRUs use gating mechanisms to control the flow of information, allowing them to capture long-term dependencies more effectively.

7. When to use RNNs, LSTMs, or GRUs?
    - **RNNs** can be used for simple sequence modeling tasks where long-term dependencies are not a concern.
    - **LSTMs** are suitable for tasks that require capturing long-term dependencies, such as language modeling or time-series forecasting.
    - **GRUs** can be a good choice when you want a simpler model that still captures long-term dependencies, especially when computational resources are limited.

8. Example applications of RNNs, LSTMs, and GRUs:
    - RNNs: Sentiment analysis, language modeling, and time-series prediction.
    - LSTMs: Machine translation, speech recognition, and text generation.
    - GRUs: Chatbots, music generation, and video analysis.
 
9. What is the hidden state in RNN?
    - The hidden state is a vector that contains information about the previous elements in the sequence. It is updated at each time step and helps the model remember past information.

10. Why does GRU train faster than LSTM?
    - GRUs have fewer parameters than LSTMs because they combine the forget and input gates into a single update gate. This simplification allows GRUs to train faster while still capturing long-term dependencies effectively.

11. What is the main difference between hidden state and cell state?
    - Hidden state → Short-term memory (used for output)
    - Cell state → Long-term memory (stores important information)

12. Why is parameter sharing important in RNNs?
    - Parameter sharing allows RNNs to use the same weights across all time steps, which helps the model generalize better and reduces the number of parameters, making it more efficient to train.

13. Why do LSTMs reduce vanishing gradients?
    - LSTMs use gating mechanisms (input, forget, and output gates) to control the flow of information and maintain a more constant error signal during backpropagation. This helps to mitigate the vanishing gradient problem and allows the model to learn long-term dependencies more effectively.

14. Why are LSTMs good for time-series forecasting?
    - LSTMs are good for time-series forecasting because they can capture long-term dependencies in the data, which is crucial for making accurate predictions based on past trends and patterns. 

15. What is the difference between RNNs and feedforward neural networks?
    - RNNs have a hidden state that allows them to process sequences of data, while feedforward neural networks do not have this capability and treat each input independently. 

16. What is the difference between LSTMs and GRUs?
    - LSTMs have three gates (input, forget, output) and a cell state, while GRUs have two gates (update and reset) and do not have a separate cell state. This makes GRUs simpler and faster to train compared to LSTMs.

17. What is the role of memory in sequence modeling?
    - Memory allows the model to retain information from previous time steps, which is essential for understanding context and making accurate predictions in sequence modeling tasks.

18. How do RNNs handle variable-length sequences?
    - RNNs can handle variable-length sequences by processing one element at a time and maintaining a hidden state that captures information from previous elements. This allows them to adapt to sequences of different lengths without requiring a fixed input size.

19. What happens if we remove the forget gate from LSTM?
    - If we remove the forget gate from LSTM, the model would not be able to effectively discard irrelevant information from the cell state. This could lead to the accumulation of unnecessary information, making it harder for the model to learn and generalize from the data.

20. If you had limited GPU memory, which would you choose: LSTM or GRU?
    - If I had limited GPU memory, I would choose GRU because it has fewer parameters than LSTM, making it more memory-efficient while still capturing long-term dependencies effectively.